---
layout: post
title: Are You Good at Your Job?
date: 2025-08-05
categories: [job]
tags: [performance, career, decision science, prediction market]
author: "Max Kossek"
description: About the difficulty measuring job performance in many fields.
sitemap:
    lastmod: 2025-08-06
---

There are no consistent, objective criteria to evaluate the performance of soft occupations in which the environment changes, fewer decisions are made, and decisions can only be evaluated over the long-term. In such fields, the sample size is too low to be able to determine those good at decision-making. A good result does not imply that the decision made was the correct one; similarly, a bad result does not mean that a bad decision was made.

The difficulty measuring job performance in soft fields is one reason why AI has had little impact on productivity statistics. AI may increase the volume and quality of work, but the evaluation and use of this data is still constrained by human review. That is increasing productivity of a knowledge worker tenfold will on average have less impact than the same productivity increase for a service or manual worker.

In general, it is a bad sign if a field evaluates performance based on peer-evaluation rather than real world consequences. Engineers are closer to ground truth and have more skin in the game than scientists. Scientists are closer to politicians, government bureaucrats, policy wonks, middle managers on the spectrum from soft jobs where you are able to BS versus "hard" jobs where you have to be serious about safety and results.

For soft fields, there has been growing interest in using prediction markets to evaluate individuals' prediction accuracy. This is an improvement over not having any criteria--or even a record--for performance evaluation. However, prediction markets still inherit the limitations of performance judgment in soft fields. Participants in the market might be good predictors in the past, but this does not mean that they will be good predictors in the future under changing, non-binary, and fat-tailed environments. In the real world, we also care about outcomes, not probabilities. Being right 99% of the time on inconsequential events can be worse than being wrong 99% of the time but right on the 1% of events that matter a lot. Or as Nassim Nicholas Taleb has written: "You do not eat forecasts, but P/L".[^1]

[^1]: Taleb, N. N. (2020). On the statistical differences between binary forecasts and real-world payoffs. International Journal of Forecasting, 36(4), 1228-1240.
