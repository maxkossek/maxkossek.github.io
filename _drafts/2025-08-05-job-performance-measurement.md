---
layout: post
title: Are You Good at Your Job?
date: 2025-08-05
categories: [job]
tags: [performance, career, decision science, prediction market]
author: "Max Kossek"
description: About the difficulty measuring job performance in many fields.
sitemap:
    lastmod: 2025-08-05
---

There are no consistent, objective criteria to evaluate the performance for "soft" occupations in which the environment changes, fewer decisions are made, and decisions can only be evaluated over the long-term. In fields with these characteristics, the sample size is too low to be able to determine those good at decision-making. A good result does not imply that the decision made was the correct one; similarly, a bad result does not mean that a bad decision was made.

In general, it is a bad sign if a field evaluates performance based on peer-evaluation rather than real world consequences. Engineers are closer to ground truth and have more skin in the game than scientists. Scientists are closer to politicians, government bureaucrats, policy wonks, middle managers on the spectrum from "soft" jobs where you are able to BS versus "hard" jobs where you have to be serious about safety and results.

For "soft" fields, there has been growing interest in using prediction markets to evaluate individuals' prediction accuracy. This is an improvement over not having any criteria--or even a record--for performance evaluation. However, prediction markets still inherit the limitations of performance judgment in soft fields. Participants in the market might have good average performance, but this does not indicate that they will necessarily be good predictors under changing, non-binary, fat-tailed environments. Or as Nassim Nicholas Taleb has written: "You do not eat forecasts, but P/L".[^1]

[^1]: Taleb, N. N. (2020). On the statistical differences between binary forecasts and real-world payoffs. International Journal of Forecasting, 36(4), 1228-1240.
